{"cells":[{"cell_type":"markdown","source":["# Before Starting\n","\n","Go to https://aistudio.google.com/app/apikey copy generative language client free tier API key\n","\n","After you did that click the key icon at the left sidebar add your API key with GOOGLE_API_KEY as name and your API key as the value and enable notebook access\n"],"metadata":{"id":"9l8Tz46_qgSi"}},{"cell_type":"markdown","metadata":{"id":"yeadDkMiISin"},"source":["# Part 2: Gemini API & Function calling with Python"]},{"cell_type":"markdown","metadata":{"id":"df1767a3d1cc"},"source":[" Function calling lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with. Function calling lets you use functions as tools in generative AI applications, and you can define more than one function within a single request.\n","\n","This notebook provides code examples to help you get started. The documentation's [quickstart](https://ai.google.dev/gemini-api/docs/function-calling#python) is also a good place to start understanding function calling."]},{"cell_type":"markdown","metadata":{"id":"hY2NtS3jV56U"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"d5027929de8f"},"source":["### Install dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"9OEoeosRTv-5","executionInfo":{"status":"ok","timestamp":1750764309307,"user_tz":-180,"elapsed":17336,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d724a85c-0d5a-4011-d364-00d5d8a98d70"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/206.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m204.8/206.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.4/206.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install -qU 'google-genai>=1.0.0'"]},{"cell_type":"markdown","metadata":{"id":"x-hHZfLZ7FfH"},"source":["### Set up your API key\n","\n","To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](../quickstarts/Authentication.ipynb) quickstart for an example."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ab9ASynfcIZn","executionInfo":{"status":"ok","timestamp":1750764311804,"user_tz":-180,"elapsed":2482,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[],"source":["from google import genai\n","from google.colab import userdata\n","\n","GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n","client = genai.Client(api_key=GOOGLE_API_KEY)"]},{"cell_type":"markdown","metadata":{"id":"zpaKynP8qLw1"},"source":["### Choose a model\n","\n","Function calling should work with all the [Gemini 2.0](https://ai.google.dev/gemini-api/docs/models/gemini-v2) models with the GenAI SDK. It also works with the 1.5 generation of models."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"sEK4ZDVGqJ5H","executionInfo":{"status":"ok","timestamp":1750764311834,"user_tz":-180,"elapsed":15,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[],"source":["MODEL_ID=\"gemini-2.0-flash-lite\" # @param [\"gemini-2.5-flash-preview-05-20\", \"gemini-2.5-pro-preview-03-25\", \"gemini-2.0-flash\", \"gemini-2.0-flash-lite\"] {\"allow-input\":true, isTemplate: true}"]},{"cell_type":"markdown","metadata":{"id":"3f383614ec30"},"source":["## Setting up Functions as Tools"]},{"cell_type":"markdown","metadata":{"id":"b82c1aecb657"},"source":["To use function calling, pass a list of functions to the `tools` parameter when creating a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel). The model uses the function name, docstring, parameters, and parameter type annotations to decide if it needs the function to best answer a prompt.\n","\n","> Important: The SDK converts function parameter type annotations to a format the API understands (`genai.types.FunctionDeclaration`). The API only supports a limited selection of parameter types, and the Python SDK's automatic conversion only supports a subset of that: `AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict`\n","\n","\n","**Example: Lighting System Functions**\n","\n","Here are 3 functions controlling a hypothetical lighting system. Note the docstrings and type hints."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"C8J_H1hSp4m-","executionInfo":{"status":"ok","timestamp":1750764359984,"user_tz":-180,"elapsed":10,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[],"source":["def enable_lights():\n","    \"\"\"Turn on the lighting system.\"\"\"\n","    print(\"LIGHTBOT: Lights enabled.\")\n","\n","\n","def set_light_color(rgb_hex: str):\n","    \"\"\"Set the light color. Lights must be enabled for this to work.\"\"\"\n","    print(f\"LIGHTBOT: Lights set to {rgb_hex}.\")\n","\n","def stop_lights():\n","    \"\"\"Stop flashing lights.\"\"\"\n","    print(\"LIGHTBOT: Lights turned off.\")\n","\n","light_controls = [enable_lights, set_light_color, stop_lights]\n","instruction = \"\"\"\n","  You are a helpful asistant that can make a variety of changes on the lights.\n","  If i ask you of something do not ask clarifying questions just fulfill the request to the best of your abilities.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"Ry0JsK405KwS"},"source":["## Basic Function Calling with Chat"]},{"cell_type":"markdown","metadata":{"id":"9l4wdq8b5Nuy"},"source":["Function calls naturally fit into multi-turn conversations. The Python SDK's `ChatSession (client.chats.create(...))` is ideal for this, as it automatically handles conversation history.\n","\n","Furthermore, `ChatSession` simplifies function calling execution via its `automatic_function_calling` feature (enabled by default), which will be explored more later. For now, let's see a basic interaction where the model decides to call a function."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"-yuQ2gCY5ujD","outputId":"ec4d5855-5475-4b61-e2ca-8c529b8d0ef4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750764367385,"user_tz":-180,"elapsed":2380,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LIGHTBOT: Lights enabled.\n","Ok, the lights are now on.\n","\n"]}],"source":["chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": light_controls,\n","        \"system_instruction\": instruction,\n","    }\n",")\n","\n","response = chat.send_message(\"It's awful dark in here...\")\n","\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"q1UsMG3FqYrC"},"source":["## Examining Function Calls and Execution History\n","\n","To understand what happened in the background, you can examine the chat history.\n","\n","The `Chat.history` property stores a chronological record of the conversation between the user and the Gemini model. You can get the history using `Chat.get_history()`. Each turn in the conversation is represented by a `genai.types.Content` object, which contains the following information:\n","\n","**Role**: Identifies whether the content originated from the \"user\" or the \"model\".\n","\n","**Parts**: A list of genai.types.Part objects that represent individual components of the message. With a text-only model, these parts can be:\n","\n","* **Text**: Plain text messages.\n","* **Function Call (genai.types.FunctionCall)**: A request from the model to execute a specific function with provided arguments.\n","* **Function Response (genai.types.FunctionResponse)**: The result returned by the user after executing the requested function.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"SBNAqSexqZlZ","outputId":"04499ed2-9b84-4ad2-f20a-f35314424096","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1750764388822,"user_tz":-180,"elapsed":53,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"It's awful dark in here..."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={} name='enable_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='enable_lights' response={'result': None} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Ok, the lights are now on.\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n"]}],"source":["from IPython.display import Markdown, display\n","\n","def print_history(chat):\n","  for content in chat.get_history():\n","      display(Markdown(\"###\" + content.role + \":\"))\n","      for part in content.parts:\n","          if part.text:\n","              display(Markdown(part.text))\n","          if part.function_call:\n","              print(\"Function call: {\", part.function_call, \"}\")\n","          if part.function_response:\n","              print(\"Function response: {\", part.function_response, \"}\")\n","      print(\"-\" * 80)\n","\n","print_history(chat)"]},{"cell_type":"markdown","metadata":{"id":"CS84-2yG7A--"},"source":["This history shows the flow:\n","\n","1. **User**: Sends the message.\n","\n","2. **Model**: Responds not with text, but with a `FunctionCall` requesting `enable_lights`.\n","\n","3. **User (SDK)**: The `ChatSession` automatically executes `enable_lights()` because `automatic_function_calling` is enabled. It sends the result back as a `FunctionResponse`.\n","\n","4. **Model**: Uses the function's result (\"Lights enabled.\") to formulate the final text response."]},{"cell_type":"markdown","metadata":{"id":"CsCZArT47p5T"},"source":["## Automatic Function Execution (Python SDK Feature)\n","\n","As demonstrated above, the `ChatSession` in the Python SDK has a powerful feature called Automatic Function Execution. When enabled (which it is by default), if the model responds with a FunctionCall, the SDK will:\n","\n","1. Find the corresponding Python function in the provided `tools`.\n","\n","2. Execute the function with the arguments provided by the model.\n","\n","3. Send the function's return value back to the model in a `FunctionResponse`.\n","\n","4. Return only the model's final response (usually text) to your code.\n","\n","This significantly simplifies the workflow for common use cases.\n","\n","**Example: Math Operations**"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"r1FnK3EB8jgQ","outputId":"57b16e47-4733-4078-dc8e-37f0db0231b3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750764519584,"user_tz":-180,"elapsed":2598,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Let the number of cats be $C = 57446845$.\n","Let the number of mittens each cat owns be $M = 4324344$.\n","To find the total number of mittens, we multiply the number of cats by the number of mittens each cat owns.\n","Total number of mittens = $C \\times M$\n","Total number of mittens = $57446845 \\times 4324344$\n","We can calculate the product:\n","$57446845 \\times 4324344 = 248284500000000 - 248284499832460$\n","Using a calculator, we have:\n","$57446845 \\times 4324344 = 248284499832460$\n","Therefore, the total number of mittens is $248284499832460$.\n","\n","Total number of mittens = $57446845 \\times 4324344 = 248284499832460$\n","\n","Final Answer: The final answer is $\\boxed{248284499832460}$\n"]}],"source":["from google.genai import types # Ensure types is imported\n","\n","def add(a: float, b: float):\n","    \"\"\"returns a + b.\"\"\"\n","    return a + b\n","\n","def subtract(a: float, b: float):\n","    \"\"\"returns a - b.\"\"\"\n","    return a - b\n","\n","def multiply(a: float, b: float):\n","    \"\"\"returns a * b.\"\"\"\n","    return a * b\n","\n","def divide(a: float, b: float):\n","    \"\"\"returns a / b.\"\"\"\n","    if b == 0:\n","        return \"Cannot divide by zero.\"\n","    return a / b\n","\n","operation_tools = [add, subtract, multiply, divide]\n","\n","chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": operation_tools,\n","        \"automatic_function_calling\": {\"disable\": False} # Enabled by default\n","    }\n",")\n","\n","response = chat.send_message(\n","    \"I have 57446845 cats, each owns 4324344 mittens, how many mittens is that in total?\"\n",")\n","\n","print(response.text)"]},{"cell_type":"code","source":["248,419,919,494,680"],"metadata":{"id":"oxYZCN1Fsl6D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a8A6qJ668ywT"},"source":["Automatic execution handled the `multiply` call seamlessly."]},{"cell_type":"markdown","metadata":{"id":"1BzsV6MxLnZD"},"source":["## Automatic Function Schema Declaration\n","\n","A key convenience of the Python SDK is its ability to automatically generate the required `FunctionDeclaration` schema from your Python functions. It inspects:\n","\n","- **Function Name**: (`func.__name__`)\n","\n","- **Docstring**: Used for the function's description.\n","\n","- **Parameters**: Names and type annotations (`int`, `str`, `float`, `bool`, `list`, `dict`). Docstrings for parameters (if using specific formats like Google style) can also enhance the description.\n","\n","- **Return Type Annotation**: Although not strictly used by the model for deciding which function to call, it's good practice.\n","\n","You generally don't need to create `FunctionDeclaration` objects manually when using Python functions directly as tools.\n","\n","However, you can generate the schema explicitly using `genai.types.FunctionDeclaration.from_callable` if you need to inspect it, modify it, or use it in scenarios where you don't have the Python function object readily available."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qrYRieAuL2hs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750764605818,"user_tz":-180,"elapsed":7,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"17a94276-8835-43ab-952f-6b7eeeae4f31"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","    \"description\": \"Set the light color. Lights must be enabled for this to work.\",\n","    \"name\": \"set_light_color\",\n","    \"parameters\": {\n","        \"properties\": {\n","            \"rgb_hex\": {\n","                \"type\": \"STRING\"\n","            }\n","        },\n","        \"required\": [\n","            \"rgb_hex\"\n","        ],\n","        \"type\": \"OBJECT\"\n","    }\n","}\n"]}],"source":["import json\n","\n","set_color_declaration = types.FunctionDeclaration.from_callable(\n","    callable = set_light_color,\n","    client = client\n",")\n","\n","print(json.dumps(set_color_declaration.to_json_dict(), indent=4))"]},{"cell_type":"code","source":["def multiply(a: float, b: float):\n","    \"\"\"Returns a * b.\"\"\"\n","    return a * b\n","\n","fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n","\n","# to_json_dict() provides a clean JSON representation.\n","print(fn_decl.to_json_dict())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEOgeosWfK5A","executionInfo":{"status":"ok","timestamp":1750576482834,"user_tz":-180,"elapsed":17,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"a2d95b04-a761-498f-a3fd-552855f1756c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'description': 'Returns a * b.', 'name': 'multiply', 'parameters': {'properties': {'a': {'type': 'NUMBER'}, 'b': {'type': 'NUMBER'}}, 'required': ['a', 'b'], 'type': 'OBJECT'}}\n"]}]},{"cell_type":"markdown","metadata":{"id":"EuwKoNIhGBJN"},"source":["## Parallel function calls\n","\n","The Gemini API can call multiple functions in a single turn. This caters for scenarios where there are multiple function calls that can take place independently to complete a task.\n","\n","First set the tools up. Unlike the movie example above, these functions do not require input from each other to be called so they should be good candidates for parallel calling."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"cJ-mSixWGqLv","executionInfo":{"status":"ok","timestamp":1750764639219,"user_tz":-180,"elapsed":4,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[],"source":["def power_disco_ball(power: bool) -> bool:\n","    \"\"\"Powers the spinning disco ball.\"\"\"\n","    print(f\"Disco ball is {'spinning!' if power else 'stopped.'}\")\n","    return True\n","\n","def start_music(energetic: bool, loud: bool, bpm: int) -> str:\n","    \"\"\"Play some music matching the specified parameters.\n","\n","    Args:\n","      energetic: Whether the music is energetic or not.\n","      loud: Whether the music is loud or not.\n","      bpm: The beats per minute of the music.\n","\n","    Returns: The name of the song being played.\n","    \"\"\"\n","    print(f\"Starting music! {energetic=} {loud=}, {bpm=}\")\n","    return \"Never gonna give you up.\"\n","\n","\n","def dim_lights(brightness: float) -> bool:\n","    \"\"\"Dim the lights.\n","\n","    Args:\n","      brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n","    \"\"\"\n","    print(f\"Lights are now set to {brightness:.0%}\")\n","    return True\n","\n","house_fns = [power_disco_ball, start_music, dim_lights]"]},{"cell_type":"markdown","metadata":{"id":"zlrmXN7fxQi0"},"source":["Now call the model with an instruction that could use all of the specified tools."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"21ecYHLgIsCl","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1750764657310,"user_tz":-180,"elapsed":11612,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"f89f1b6a-343d-44d9-f31b-eda3d8a628dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Disco ball is spinning!\n","Starting music! energetic=True loud=True, bpm=120\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n","Disco ball is spinning!\n","Starting music! energetic=True loud=True, bpm=120\n","Lights are now set to 20%\n","Disco ball is spinning!\n","Starting music! energetic=True loud=True, bpm=120\n","Lights are now set to 20%\n","Disco ball is spinning!\n","Starting music! energetic=True loud=True, bpm=120\n","Lights are now set to 20%\n","Disco ball is spinning!\n","Starting music! energetic=True loud=True, bpm=120\n","Lights are now set to 20%\n","Disco ball is spinning!\n","Starting music! energetic=True loud=True, bpm=120\n","Lights are now set to 20%\n","Disco ball is spinning!\n","Starting music! energetic=True loud=True, bpm=120\n","Lights are now set to 20%\n","Disco ball is spinning!\n","Starting music! energetic=True loud=True, bpm=120\n","Lights are now set to 20%\n","Disco ball is spinning!\n","Starting music! energetic=True loud=True, bpm=120\n","Lights are now set to 20%\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Turn this place into a party!"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'power': True} name='power_disco_ball' }\n","Function call: { id=None args={'loud': True, 'energetic': True, 'bpm': 120} name='start_music' }\n","Function call: { id=None args={'brightness': 0.2} name='dim_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='power_disco_ball' response={'result': True} }\n","Function response: { will_continue=None scheduling=None id=None name='start_music' response={'result': 'Never gonna give you up.'} }\n","Function response: { will_continue=None scheduling=None id=None name='dim_lights' response={'result': True} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'loud': True, 'bpm': 120, 'energetic': True} name='start_music' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='start_music' response={'result': 'Never gonna give you up.'} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'power': True} name='power_disco_ball' }\n","Function call: { id=None args={'loud': True, 'bpm': 120, 'energetic': True} name='start_music' }\n","Function call: { id=None args={'brightness': 0.2} name='dim_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='power_disco_ball' response={'result': True} }\n","Function response: { will_continue=None scheduling=None id=None name='start_music' response={'result': 'Never gonna give you up.'} }\n","Function response: { will_continue=None scheduling=None id=None name='dim_lights' response={'result': True} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'power': True} name='power_disco_ball' }\n","Function call: { id=None args={'loud': True, 'bpm': 120, 'energetic': True} name='start_music' }\n","Function call: { id=None args={'brightness': 0.2} name='dim_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='power_disco_ball' response={'result': True} }\n","Function response: { will_continue=None scheduling=None id=None name='start_music' response={'result': 'Never gonna give you up.'} }\n","Function response: { will_continue=None scheduling=None id=None name='dim_lights' response={'result': True} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'power': True} name='power_disco_ball' }\n","Function call: { id=None args={'loud': True, 'energetic': True, 'bpm': 120} name='start_music' }\n","Function call: { id=None args={'brightness': 0.2} name='dim_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='power_disco_ball' response={'result': True} }\n","Function response: { will_continue=None scheduling=None id=None name='start_music' response={'result': 'Never gonna give you up.'} }\n","Function response: { will_continue=None scheduling=None id=None name='dim_lights' response={'result': True} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'power': True} name='power_disco_ball' }\n","Function call: { id=None args={'loud': True, 'bpm': 120, 'energetic': True} name='start_music' }\n","Function call: { id=None args={'brightness': 0.2} name='dim_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='power_disco_ball' response={'result': True} }\n","Function response: { will_continue=None scheduling=None id=None name='start_music' response={'result': 'Never gonna give you up.'} }\n","Function response: { will_continue=None scheduling=None id=None name='dim_lights' response={'result': True} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'power': True} name='power_disco_ball' }\n","Function call: { id=None args={'loud': True, 'energetic': True, 'bpm': 120} name='start_music' }\n","Function call: { id=None args={'brightness': 0.2} name='dim_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='power_disco_ball' response={'result': True} }\n","Function response: { will_continue=None scheduling=None id=None name='start_music' response={'result': 'Never gonna give you up.'} }\n","Function response: { will_continue=None scheduling=None id=None name='dim_lights' response={'result': True} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'power': True} name='power_disco_ball' }\n","Function call: { id=None args={'energetic': True, 'loud': True, 'bpm': 120} name='start_music' }\n","Function call: { id=None args={'brightness': 0.2} name='dim_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='power_disco_ball' response={'result': True} }\n","Function response: { will_continue=None scheduling=None id=None name='start_music' response={'result': 'Never gonna give you up.'} }\n","Function response: { will_continue=None scheduling=None id=None name='dim_lights' response={'result': True} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'power': True} name='power_disco_ball' }\n","Function call: { id=None args={'energetic': True, 'bpm': 120, 'loud': True} name='start_music' }\n","Function call: { id=None args={'brightness': 0.2} name='dim_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='power_disco_ball' response={'result': True} }\n","Function response: { will_continue=None scheduling=None id=None name='start_music' response={'result': 'Never gonna give you up.'} }\n","Function response: { will_continue=None scheduling=None id=None name='dim_lights' response={'result': True} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'power': True} name='power_disco_ball' }\n","Function call: { id=None args={'energetic': True, 'bpm': 120, 'loud': True} name='start_music' }\n","Function call: { id=None args={'brightness': 0.2} name='dim_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='power_disco_ball' response={'result': True} }\n","Function response: { will_continue=None scheduling=None id=None name='start_music' response={'result': 'Never gonna give you up.'} }\n","Function response: { will_continue=None scheduling=None id=None name='dim_lights' response={'result': True} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={'power': True} name='power_disco_ball' }\n","Function call: { id=None args={'energetic': True, 'bpm': 120, 'loud': True} name='start_music' }\n","Function call: { id=None args={'brightness': 0.2} name='dim_lights' }\n","--------------------------------------------------------------------------------\n"]}],"source":["# You generally set \"mode\": \"any\" to make sure Gemini actually *uses* the given tools.\n","party_chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": house_fns,\n","        \"tool_config\" : {\n","            \"function_calling_config\": {\n","                \"mode\": \"any\"\n","            }\n","        }\n","    }\n",")\n","\n","# Call the API\n","response = party_chat.send_message(\n","    \"Turn this place into a party!\"\n",")\n","\n","\n","print_history(party_chat)"]},{"cell_type":"markdown","metadata":{"id":"t6iYpty7yZct"},"source":["Notice the single model turn contains three FunctionCall parts, which the SDK then executed before getting the final text response."]},{"cell_type":"markdown","metadata":{"id":"TxXGT3n4AQhk"},"source":["## Compositional Function Calling\n","The model can chain function calls across multiple turns, using the result from one call to inform the next. This allows for complex, multi-step reasoning and task completion.\n","\n","**Example: Finding Specific Movie Showtimes**\n","\n","Let's reuse the theater_functions and ask a more complex query that requires finding movies first, then potentially theaters, then showtimes."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"1jGiexKsAolU","outputId":"edeb3f51-a753-45a6-b9f7-b8b438368891","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750764690777,"user_tz":-180,"elapsed":5250,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tool Call: get_weather_forecast(location=London)\n","Tool Response: {'temperature': 25, 'unit': 'celsius'}\n","Tool Call: set_thermostat_temperature(temperature=20)\n","Tool Response: {'status': 'success'}\n","OK. I've set the thermostat to 20°C.\n","\n"]}],"source":["import os\n","\n","from google.colab import userdata\n","GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n","\n","# Example Functions\n","def get_weather_forecast(location: str) -> dict:\n","    \"\"\"Gets the current weather temperature for a given location.\"\"\"\n","    print(f\"Tool Call: get_weather_forecast(location={location})\")\n","    # TODO: Make API call\n","    print(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\")\n","    return {\"temperature\": 25, \"unit\": \"celsius\"}  # Dummy response\n","\n","def set_thermostat_temperature(temperature: int) -> dict:\n","    \"\"\"Sets the thermostat to a desired temperature.\"\"\"\n","    print(f\"Tool Call: set_thermostat_temperature(temperature={temperature})\")\n","\n","    print(\"Tool Response: {'status': 'success'}\")\n","    return {\"status\": \"success\"}\n","\n","client = genai.Client(api_key = GOOGLE_API_KEY)\n","config = types.GenerateContentConfig(\n","    tools=[get_weather_forecast, set_thermostat_temperature]\n",")\n","\n","\n","response = client.models.generate_content(\n","    model=\"gemini-2.0-flash\",\n","    contents=\"If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.\",\n","    config=config,\n",")\n","\n","\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"s7J-nyTN8hL-"},"source":["Here you can see that the model made conditional calls.\n"]},{"cell_type":"markdown","source":["# Exercises"],"metadata":{"id":"g2rNsyqzi_0S"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"mggqLU55CZlj","colab":{"base_uri":"https://localhost:8080/","height":371,"referenced_widgets":["a9d89ffd59f64fe395154b4beb280cc6","23f17a608c7c4e37aec684e8a85f1ec7","a78c83d884794c49acb441058cc06267","e8ccb34d3740430d9cfd8486651c44db","80f78aadcf2e4224bd05445dc815a47e","503d0715882948d4911438578b298ab6","84b380da7ed54e6b9e63a6e4d32d741c","0f88acbd5c144d108fedcdc11c66e9a7","39632b5096d347f0833f3ad0c968cd8f","7a047901fc114592a37da8d5b4ffd0a6","589e9a28925a47628cd636988afe2cbe","b62eff6fec064d55b3951ff1be8b6e42","eb6f8efeaa784aa6af9637ec412ed6a9","2f7956f884564b3e9539fe5ca6d380df","bf8911c2d422444e9a3f1acfb6b4e0cb","6e610afa99324dba8ce5d77fa725fba6","97dc12d6a7da453b82251ad5bb3310cd"]},"executionInfo":{"status":"ok","timestamp":1750765117916,"user_tz":-180,"elapsed":1083,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"b7fa8847-1be1-44af-82a3-b8ab9cf50367"},"outputs":[{"output_type":"display_data","data":{"text/plain":["HBox(children=(VBox(children=(Output(layout=Layout(border='1px solid #ccc', height='320px', overflow='auto')),…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d89ffd59f64fe395154b4beb280cc6"}},"metadata":{}}],"source":["import re\n","from IPython.display import display\n","import ipywidgets as widgets\n","import google.generativeai as genai\n","from google.colab import userdata\n","\n","GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n","\n","\n","instructions = \"\"\"\n","You are a helpful and friendly AI assistant that controls a smart light bulb.\n","Your only capabilities are to turn the light on, turn it off, and change its color.\n","\n","- To turn the light on, use the `turn_on_lights` function.\n","- To turn it off, use the `turn_off_lights` function.\n","- To change the color, use the `set_light_color` function.\n","\n","When setting the color, you MUST use one of the following predefined names or a standard 6-digit hex color code (e.g., #FF5733).\n","Available color names: gold, warm, white, blue, green, red, purple.\n","\n","Do not mention the lights yourself only estimate users intentions\n","\"\"\"\n","\n","chat_output = widgets.Output(\n","    layout=widgets.Layout(border=\"1px solid #ccc\", height=\"320px\", overflow=\"auto\")\n",")\n","user_input = widgets.Text(\n","    placeholder=\"Type your message and press Enter…\",\n","    layout=widgets.Layout(flex=\"1 1 auto\")\n",")\n","send_button = widgets.Button(description=\"Send\", button_style=\"primary\")\n","\n","SVG_TEMPLATE = (\n","    \"<div style='display:flex;flex-direction:column;align-items:center;justify-content:center;height:100%;'>\"\n","    \"<svg width='120' height='120' viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'>\"\n","    \"  <path d='M12 2c-4 0-7 3-7 7 0 3 2 4 2 7h10c0-3 2-4 2-7 0-4-3-7-7-7z' fill='{color}' stroke='#333' stroke-width='0.5'/>\"\n","    \"  <rect x='9' y='18' width='6' height='2' fill='{color}' />\"\n","    \"  <rect x='10.5' y='21' width='3' height='2' fill='{color}' />\"\n","    \"</svg>\"\n","    \"<span style='margin-top:8px;font-size:14px;'>{msg}</span>\"\n","    \"</div>\"\n",")\n","\n","bulb_output = widgets.HTML(\n","    value=SVG_TEMPLATE.format(color=\"#666\", msg=\"The light is OFF.\"),\n","    layout=widgets.Layout(border=\"1px solid #ccc\", width=\"220px\", height=\"320px\")\n",")\n","\n","# ---------------- State ----------------\n","light_is_on = False\n","\n","# Default ON colour (Gold) and palette\n","current_on_color = COLOR_MAP[\"gold\"]\n","\n","# ---------------- Helper ----------------\n","\n","def _update_bulb(is_on: bool, msg: str):\n","    \"\"\"Re‑render bulb in its current colour plus caption.\"\"\"\n","    global light_is_on\n","    light_is_on = is_on\n","    color = current_on_color if is_on else \"#666\"  # dark grey when off\n","    bulb_output.value = SVG_TEMPLATE.format(color=color, msg=msg)\n","\n","# ---------------- Tool Functions ----------------\n","\n","def turn_on_lights():\n","    \"\"\"Turn the light on\"\"\"\n","    if light_is_on:\n","        _update_bulb(True, \"The lights were already ON.\")\n","        return {\"status\": \"The lights were already ON.\"}\n","    _update_bulb(True, \"The lights have been turned ON.\")\n","    return {\"status\": \"The lights have been turned ON.\"}\n","\n","\n","def turn_off_lights():\n","    \"\"\"Turn off the lights\"\"\"\n","    if not light_is_on:\n","        _update_bulb(False, \"The lights were already OFF.\")\n","        return {\"status\": \"The lights were already OFF.\"}\n","    _update_bulb(False, \"The lights have been turned OFF.\")\n","    return {\"status\": \"The lights have been turned OFF.\"}\n","\n","\n","def set_light_color(hexrgb: str):\n","    \"\"\"\n","    Change the ON color of the light bulb.\n","    Only accepts standard 6-digit hex color strings (with or without a leading '#').\n","    \"\"\"\n","    global current_on_color  # Declare that we're modifying the global variable `current_on_color`\n","\n","    # Strip any leading/trailing whitespace and convert to lowercase (e.g., \" #FF00FF \" -> \"#ff00ff\")\n","    hexrgb = hexrgb.strip().lower()\n","\n","    # Validate input using a regular expression:\n","    # - optional '#' at the beginning\n","    # - exactly six hexadecimal digits (0-9, a-f)\n","    if re.fullmatch(r\"#?[0-9a-f]{6}\", hexrgb):\n","        # If it matches, remove leading '#' if present and add it back explicitly\n","        current_on_color = \"#\" + hexrgb.lstrip(\"#\")\n","\n","        # Build a success status message\n","        status = f\"Custom colour {current_on_color} applied.\"\n","    else:\n","        # If input doesn't match valid hex format, return an error message\n","        status = \"Invalid colour. Please enter a hex value like #ff0000 or ff0000.\"\n","        return {\"status\": status}\n","\n","    # If the light is currently ON, re-render the SVG bulb immediately using the new color\n","    _update_bulb(light_is_on, status)\n","\n","    # Return status message as a dictionary (used for function call response)\n","    return {\"status\": status}\n","\n","\n","# ---------------- Gemini 2.0 Flash Setup ----------------\n","light_controls = [turn_on_lights, turn_off_lights, set_light_color]\n","genai.configure(api_key=GOOGLE_API_KEY)\n","model = genai.GenerativeModel(\n","    model_name=\"gemini-2.5-flash-lite-preview-06-17\",\n","    tools=light_controls,\n","    system_instruction=instruction\n",")\n","chat = model.start_chat(enable_automatic_function_calling=True)\n","\n","# ---------------- Chat Handling ----------------\n","\n","def _append_chat(role: str, text: str):\n","    with chat_output:\n","        print(f\"{role}: {text}\\n\")\n","\n","@chat_output.capture()\n","def _send(_=None):\n","    prompt = user_input.value.strip()\n","    if not prompt:\n","        return\n","    _append_chat(\"👤 You\", prompt)\n","    user_input.value = \"\"\n","    try:\n","        response = chat.send_message(prompt)\n","        _append_chat(\"🤖 Gemini\", response.text)\n","    except Exception as e:\n","        _append_chat(\"⚠️ Error\", str(e))\n","\n","# ---------------- Bind UI ----------------\n","send_button.on_click(_send)\n","user_input.on_submit(_send)\n","\n","ui = widgets.HBox([\n","    widgets.VBox([chat_output, widgets.HBox([user_input, send_button])], layout=widgets.Layout(flex=\"1 1 0%\")),\n","    bulb_output\n","])\n","\n","display(ui)\n","\n"]},{"cell_type":"code","source":["# ╔═══════════════ Exercise 1 — Basic Math  ═══════════════╗\n","def add(a: float, b: float):\n","    \"\"\"Returns a + b.\"\"\"\n","    return a + b\n","\n","def multiply(a: float, b: float):\n","    \"\"\"Returns a * b.\"\"\"\n","    return a * b\n","\n","calc_tools = [add, multiply]\n","chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": calc_tools,\n","        \"system_instruction\": instruction,\n","    }\n",")\n","\n","response = chat.send_message(\"What’s 7 × 8?  Also, what’s 5 + 4?\")\n","\n","print(response.text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l4FHY8wskcUt","outputId":"6311ac73-99d1-499e-ecae-3b46baf90e93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7 times 8 is 56, and 5 plus 4 is 9.\n","\n"]}]},{"cell_type":"code","source":["# ╔═══════════════ Exercise 2 — Temperature Converter  ═══════════════╗\n","import google.generativeai as genai\n","from google.generativeai import types\n","\n","# ——— solution functions ——— define your functions here\n","def f_to_c(fahreneit: float):\n","    \"\"\"Convert fahrenheit to celcius\"\"\"\n","    return fahreneit - 32 * (5/9)\n","\n","temp_tools = [f_to_c, c_to_f]\n","chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": temp_tools,\n","        \"system_instruction\": instruction,\n","    }\n",")\n","\n","response = chat.send_message(\"Convert 72 °F to Celsius.\")\n","\n","print(response.text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"swARCEYslNbk","outputId":"bf8e21f4-c0b1-463c-851f-b985f3554db2","executionInfo":{"status":"error","timestamp":1750765654464,"user_tz":-180,"elapsed":157,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"execution_count":17,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'c_to_f' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-17-1363939285.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"\"\"Convert fahrenheit to celcius\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtemp_tools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf_to_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_to_f\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m chat = client.chats.create(\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'c_to_f' is not defined"]}]},{"cell_type":"code","source":["# ╔═══════════════ Exercise 3 — String Helper  ═══════════════╗\n","import google.generativeai as genai\n","from google.generativeai import types\n","\n","# ——— solution functions ——— define your functions here\n","\n","def reverse(mystr : str)\n","    \"\"\"Reverses a string\"\"\"\n","    return mystr[::-1]\n","\n","# ——— Gemini wiring & quick demo ———\n","string_tools = [reverse, count_vowels]\n","chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": string_tools,\n","        \"system_instruction\": instruction,\n","    }\n",")\n","\n","\n","response = chat.send_message(\"How many vowels are in 'function calling'? Also give me the text reversed.\")\n","\n","print(response.text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7QUCDHkHlOfk","outputId":"3ea7d09f-5561-46aa-bb6e-b1c9bafe01fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 5 vowels in 'function calling'. The reversed text is 'gnillac noitcnuf'.\n","\n"]}]},{"cell_type":"markdown","source":["# Flexibility"],"metadata":{"id":"3OoTUwm-ynKr"}},{"cell_type":"code","source":["import pandas as pd\n","import ast\n","import io, contextlib, traceback, re\n","from IPython.display import display, HTML\n","\n","MODEL_ID=\"gemini-2.5-flash-lite-preview-06-17\"\n","\n","client = genai.Client(api_key=GOOGLE_API_KEY)\n"],"metadata":{"id":"ebDnVJKr5jus"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sales_data = [\n","    ('Electronics', 3, 150.00),\n","    ('Books', 10, 12.50),\n","    ('Electronics', 7, 200.00),\n","    ('Food', 2, 5.00),\n","    ('Books', 4, 10.00),\n","    ('Electronics', 1, 300.00),\n","    ('Food', 8, 7.50),\n","    ('Books', 6, 15.00),\n","    ('Clothing', 12, 25.00),\n","    ('Food', 9, 6.00),\n","]\n","\n","print(\"--- Original Sales Data ---\")\n","display(pd.DataFrame(sales_data, columns=['Category', 'Quantity', 'UnitPrice']))\n","\n","# Manually calculate the correct solution for comparison\n","correct_solution_sales = [\n","    ('Books', 215.00),\n","    ('Clothing', 300.00),\n","    ('Electronics', 1400.00),\n","    ('Food', 114.00),\n","]\n","\n","print(\"\\n--- Correct Solution for Sales Data ---\")\n","print(correct_solution_sales)\n","\n","\n","# ╔═══════════════ Tool Definition for Tool-Calling Scenario ═══════════════╗\n","\n","def execute_python_code(code: str) -> str:\n","    \"\"\"Executes the given Python code and returns its stdout or a traceback.\"\"\"\n","    buf = io.StringIO()\n","    with contextlib.redirect_stdout(buf):\n","        try:\n","            exec(code, {})\n","        except Exception:\n","            traceback.print_exc(file=buf)\n","    return buf.getvalue()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"id":"_VHzJpzd9p16","executionInfo":{"status":"ok","timestamp":1750701977734,"user_tz":-180,"elapsed":51,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"f2b16fde-a32f-449f-ff6b-8127ac48aeeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Original Sales Data ---\n"]},{"output_type":"display_data","data":{"text/plain":["      Category  Quantity  UnitPrice\n","0  Electronics         3      150.0\n","1        Books        10       12.5\n","2  Electronics         7      200.0\n","3         Food         2        5.0\n","4        Books         4       10.0\n","5  Electronics         1      300.0\n","6         Food         8        7.5\n","7        Books         6       15.0\n","8     Clothing        12       25.0\n","9         Food         9        6.0"],"text/html":["\n","  <div id=\"df-1593314c-54c1-47ef-b92f-b880c57c2b11\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Category</th>\n","      <th>Quantity</th>\n","      <th>UnitPrice</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Electronics</td>\n","      <td>3</td>\n","      <td>150.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Books</td>\n","      <td>10</td>\n","      <td>12.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Electronics</td>\n","      <td>7</td>\n","      <td>200.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Food</td>\n","      <td>2</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Books</td>\n","      <td>4</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Electronics</td>\n","      <td>1</td>\n","      <td>300.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Food</td>\n","      <td>8</td>\n","      <td>7.5</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Books</td>\n","      <td>6</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Clothing</td>\n","      <td>12</td>\n","      <td>25.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Food</td>\n","      <td>9</td>\n","      <td>6.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1593314c-54c1-47ef-b92f-b880c57c2b11')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1593314c-54c1-47ef-b92f-b880c57c2b11 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1593314c-54c1-47ef-b92f-b880c57c2b11');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-f50e4651-61f7-4bad-a64a-637a32e0d9cb\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f50e4651-61f7-4bad-a64a-637a32e0d9cb')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-f50e4651-61f7-4bad-a64a-637a32e0d9cb button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"    return buf\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Books\",\n          \"Clothing\",\n          \"Electronics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quantity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          12,\n          10,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UnitPrice\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 105.56061976155902,\n        \"min\": 5.0,\n        \"max\": 300.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          25.0,\n          12.5,\n          300.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- Correct Solution for Sales Data ---\n","[('Books', 215.0), ('Clothing', 300.0), ('Electronics', 1400.0), ('Food', 114.0)]\n"]}]},{"cell_type":"code","source":["# ╔═══════════════ Scenario 1: Model’s Direct Solution ═══════════════╗\n","\n","chat_direct = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"system_instruction\": (\n","            \"Given a list of sales records [('category', quantity, unit_price)], \"\n","            \"calculate the total revenue for each product category ONLY for sales where more than 5 units were sold. \"\n","            \"Return the results as a Python list of (category, total_revenue) tuples, sorted alphabetically by category name. \"\n","            \"Round total revenue to two decimal places. \"\n","            \"Do NOT output ```python ``` or any other text, ONLY the list.\"\n","        ),\n","        \"temperature\": 0,\n","        \"tools\": [], # No tools for this chat\n","        \"automatic_function_calling\": {\"disable\": True}\n","    }\n",")\n","\n","problem_description = \"Sales data: \" + str(sales_data) + \".\"\n","response_direct = chat_direct.send_message(problem_description)\n","model_direct_solution_text = response_direct.text.strip()\n","\n","print(\"\\n--- Scenario 1: Model’s Direct Solution (raw text) ---\")\n","print(model_direct_solution_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njBaT34b9txq","executionInfo":{"status":"ok","timestamp":1750701978001,"user_tz":-180,"elapsed":242,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"d720a2ee-cd8d-4a0f-df6e-255bb6ff4eb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Scenario 1: Model’s Direct Solution (raw text) ---\n","[('Books', 225.0), ('Clothing', 300.0), ('Electronics', 1700.0), ('Food', 112.5)]\n"]}]},{"cell_type":"code","source":["# ╔═══════════════ Scenario 2: Model (Tool-Calling Execution) ═══════════════╗\n","\n","chat_tool_calling = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"system_instruction\": (\n","            \"You are a helpful assistant that can perform calculations. \"\n","            \"You have a tool `execute_python_code` that can run Python code. \"\n","            \"Given a list of sales records, calculate the total revenue for each product category ONLY for sales where more than 5 units were sold. \"\n","            \"Round total revenue to two decimal places. \"\n","            \"Once you have the final aggregated results from the tool, output ONLY the final Python list of (category, total_revenue) tuples, \"\n","            \"sorted alphabetically by category name. Do NOT output any other text or markdown fences around the final list.\"\n","        ),\n","        \"temperature\": 0,\n","        \"tools\": [execute_python_code], # Tool is registered here\n","        \"automatic_function_calling\": {\"disable\": False} # Tool calling is enabled\n","    }\n",")\n","\n","response_tool_calling = chat_tool_calling.send_message(problem_description)\n","tool_exec_solution_text = response_tool_calling.text.strip()\n","\n","# Fallback in case the model doesn't provide a final summary\n","if not tool_exec_solution_text:\n","    print(\"Model did not provide a final text summary. Looking for tool output directly...\")\n","    try:\n","        for part in response_tool_calling.candidates[0].content.parts:\n","            if part.function_response:\n","                tool_exec_solution_text = part.function_response.response['output'].strip()\n","                break\n","    except (IndexError, AttributeError):\n","        tool_exec_solution_text = \"\"\n","\n","print(\"\\n--- Scenario 2: Model (Tool-Calling Execution) Solution (processed by model) ---\")\n","print(tool_exec_solution_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QogO8N8K61eP","executionInfo":{"status":"ok","timestamp":1750701978879,"user_tz":-180,"elapsed":875,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"f2608c5c-b8ed-4b3d-f4d6-ed6840d48b56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Scenario 2: Model (Tool-Calling Execution) Solution (processed by model) ---\n","[('Books', 215.0), ('Clothing', 300.0), ('Electronics', 1400.0), ('Food', 114.0)]\n"]}]},{"cell_type":"code","source":["# ╔═══════════════ Comparison & Enhanced Visualization ═══════════════╗\n","\n","def parse_and_standardize_solution(solution_text, source_name):\n","    \"\"\"\n","    Parses a string into a list of (category, revenue) tuples and converts it\n","    to a dictionary for easier lookup, handling potential parsing errors.\n","    \"\"\"\n","    try:\n","        parsed_list = ast.literal_eval(solution_text)\n","        if not isinstance(parsed_list, list):\n","            print(f\"Warning: {source_name} output is not a list after parsing.\")\n","            return {}, 1\n","\n","        standardized_dict = {}\n","        for item in parsed_list:\n","            if isinstance(item, (list, tuple)) and len(item) == 2:\n","                category = item[0]\n","                revenue = round(float(item[1]), 2)\n","                standardized_dict[category] = revenue\n","            else:\n","                print(f\"Warning: {source_name} output contains malformed item: {item}\")\n","                return {}, 1\n","        return standardized_dict, 0\n","    except (ValueError, SyntaxError) as e:\n","        print(f\"Error parsing {source_name}'s output: {e}. Raw output: '{solution_text}'\")\n","        return {}, 1\n","\n","def is_value_correct(val, correct_val):\n","    \"\"\"Checks if a given value matches the correct value, handling NAs and float tolerance.\"\"\"\n","    epsilon = 0.01\n","    if pd.isna(correct_val):\n","        return pd.isna(val)\n","    elif pd.isna(val):\n","        return False\n","    elif abs(val - correct_val) < epsilon:\n","        return True\n","    else:\n","        return False\n","\n","# Parse all solutions\n","correct_dict, _ = parse_and_standardize_solution(str(correct_solution_sales), \"Correct Solution\")\n","model_direct_dict, model_direct_parse_error = parse_and_standardize_solution(model_direct_solution_text, \"Model (Direct)\")\n","tool_exec_dict, tool_exec_parse_error = parse_and_standardize_solution(tool_exec_solution_text, \"Model (Tool-Calling Exec)\")\n","\n","# Collect all unique categories\n","all_categories = sorted(list(set(\n","    list(correct_dict.keys()) +\n","    list(model_direct_dict.keys()) +\n","    list(tool_exec_dict.keys())\n",")))\n","\n","comparison_data = []\n","errors = {\n","    \"Model (Direct)\": 0,\n","    \"Model (Tool-Calling Exec)\": 0\n","}\n","\n","for cat in all_categories:\n","    correct_val = correct_dict.get(cat, pd.NA)\n","\n","    model_direct_val = model_direct_dict.get(cat, pd.NA)\n","    tool_exec_val = tool_exec_dict.get(cat, pd.NA)\n","\n","    model_direct_is_correct = is_value_correct(model_direct_val, correct_val)\n","    if not model_direct_is_correct:\n","        errors[\"Model (Direct)\"] += 1\n","\n","    tool_exec_is_correct = is_value_correct(tool_exec_val, correct_val)\n","    if not tool_exec_is_correct:\n","        errors[\"Model (Tool-Calling Exec)\"] += 1\n","\n","    comparison_data.append({\n","        'Category': cat,\n","        'Correct Value': correct_val,\n","        'Model (Direct)': model_direct_val,\n","        'Model (Direct) Correct': '✓' if model_direct_is_correct else '✗',\n","        'Model (Tool-Calling Exec)': tool_exec_val,\n","        'Model (Tool-Calling Exec) Correct': '✓' if tool_exec_is_correct else '✗',\n","    })\n","\n","if model_direct_parse_error:\n","    errors[\"Model (Direct)\"] += len(correct_dict)\n","if tool_exec_parse_error:\n","    errors[\"Model (Tool-Calling Exec)\"] += len(correct_dict)\n","\n","comparison_df = pd.DataFrame(comparison_data)\n","\n","def highlight_errors(row):\n","    styles = [''] * len(row)\n","\n","    if row['Model (Direct) Correct'] == '✗':\n","        styles[comparison_df.columns.get_loc('Model (Direct)')] = 'background-color: #ffcccc'\n","    if row['Model (Tool-Calling Exec) Correct'] == '✗':\n","        styles[comparison_df.columns.get_loc('Model (Tool-Calling Exec)')] = 'background-color: #ffcccc'\n","\n","    return styles\n","\n","print(\"\\n--- Detailed Comparison ---\")\n","styled_df = comparison_df.style.apply(highlight_errors, axis=1)\n","display(styled_df)\n","\n","print(f\"\\nSummary:\")\n","print(f\"Model (Direct) got {errors['Model (Direct)']} incorrect/missing categories.\")\n","print(f\"Model (Tool-Calling Exec) got {errors['Model (Tool-Calling Exec)']} incorrect/missing categories.\")\n","\n","if errors['Model (Direct)'] == errors['Model (Tool-Calling Exec)']:\n","    if errors['Model (Direct)'] == 0:\n","        print(\"\\nConclusion: Both methods provided a perfect solution.\")\n","    else:\n","        print(\"\\nConclusion: Both methods performed equally.\")\n","elif errors['Model (Direct)'] > errors['Model (Tool-Calling Exec)']:\n","    print(\"\\nConclusion: The Model with Tool-Calling Execution performed better.\")\n","else:\n","    print(\"\\nConclusion: The Direct Model performed better.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"exD_2g2b9xnb","executionInfo":{"status":"ok","timestamp":1750701978921,"user_tz":-180,"elapsed":28,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"8d8e6c0c-b7ab-444a-e39a-9e55f8b0ffc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Detailed Comparison ---\n"]},{"output_type":"display_data","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7ed90e77d290>"],"text/html":["<style type=\"text/css\">\n","#T_d793d_row0_col2, #T_d793d_row2_col2, #T_d793d_row3_col2 {\n","  background-color: #ffcccc;\n","}\n","</style>\n","<table id=\"T_d793d\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_d793d_level0_col0\" class=\"col_heading level0 col0\" >Category</th>\n","      <th id=\"T_d793d_level0_col1\" class=\"col_heading level0 col1\" >Correct Value</th>\n","      <th id=\"T_d793d_level0_col2\" class=\"col_heading level0 col2\" >Model (Direct)</th>\n","      <th id=\"T_d793d_level0_col3\" class=\"col_heading level0 col3\" >Model (Direct) Correct</th>\n","      <th id=\"T_d793d_level0_col4\" class=\"col_heading level0 col4\" >Model (Tool-Calling Exec)</th>\n","      <th id=\"T_d793d_level0_col5\" class=\"col_heading level0 col5\" >Model (Tool-Calling Exec) Correct</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_d793d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_d793d_row0_col0\" class=\"data row0 col0\" >Books</td>\n","      <td id=\"T_d793d_row0_col1\" class=\"data row0 col1\" >215.000000</td>\n","      <td id=\"T_d793d_row0_col2\" class=\"data row0 col2\" >225.000000</td>\n","      <td id=\"T_d793d_row0_col3\" class=\"data row0 col3\" >✗</td>\n","      <td id=\"T_d793d_row0_col4\" class=\"data row0 col4\" >215.000000</td>\n","      <td id=\"T_d793d_row0_col5\" class=\"data row0 col5\" >✓</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d793d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_d793d_row1_col0\" class=\"data row1 col0\" >Clothing</td>\n","      <td id=\"T_d793d_row1_col1\" class=\"data row1 col1\" >300.000000</td>\n","      <td id=\"T_d793d_row1_col2\" class=\"data row1 col2\" >300.000000</td>\n","      <td id=\"T_d793d_row1_col3\" class=\"data row1 col3\" >✓</td>\n","      <td id=\"T_d793d_row1_col4\" class=\"data row1 col4\" >300.000000</td>\n","      <td id=\"T_d793d_row1_col5\" class=\"data row1 col5\" >✓</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d793d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_d793d_row2_col0\" class=\"data row2 col0\" >Electronics</td>\n","      <td id=\"T_d793d_row2_col1\" class=\"data row2 col1\" >1400.000000</td>\n","      <td id=\"T_d793d_row2_col2\" class=\"data row2 col2\" >1700.000000</td>\n","      <td id=\"T_d793d_row2_col3\" class=\"data row2 col3\" >✗</td>\n","      <td id=\"T_d793d_row2_col4\" class=\"data row2 col4\" >1400.000000</td>\n","      <td id=\"T_d793d_row2_col5\" class=\"data row2 col5\" >✓</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d793d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_d793d_row3_col0\" class=\"data row3 col0\" >Food</td>\n","      <td id=\"T_d793d_row3_col1\" class=\"data row3 col1\" >114.000000</td>\n","      <td id=\"T_d793d_row3_col2\" class=\"data row3 col2\" >112.500000</td>\n","      <td id=\"T_d793d_row3_col3\" class=\"data row3 col3\" >✗</td>\n","      <td id=\"T_d793d_row3_col4\" class=\"data row3 col4\" >114.000000</td>\n","      <td id=\"T_d793d_row3_col5\" class=\"data row3 col5\" >✓</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Summary:\n","Model (Direct) got 3 incorrect/missing categories.\n","Model (Tool-Calling Exec) got 0 incorrect/missing categories.\n","\n","Conclusion: The Model with Tool-Calling Execution performed better.\n"]}]},{"cell_type":"markdown","source":["# ╔═════════ Best-Practice Cheat-Sheet ═══════════╗\n","\n","### Best-Practice Checklist for Gemini Function Calling 🔧\n","\n","| Area | What to Do | Why It Matters |\n","|------|------------|----------------|\n","| **Function & parameter descriptions** | • Write *explicit, unambiguous* docstrings.<br>• Spell out units, ranges, and edge cases. | The model chooses the tool—and its arguments—based on these strings alone. |\n","| **Naming** | • Use descriptive snake-case names: `add_user`, `fetch_fx_rate`.<br>• Avoid spaces, periods, or dashes. | Clear names lower the odds of the wrong tool being picked. |\n","| **Strong typing** | • Annotate every parameter (`int`, `str`, etc.).<br>• For limited choices, declare an **enum**. | Tighter schemas ⇒ fewer invalid calls. |\n","| **Tool selection** | • Keep the *active* tool set ≤ 10-20.<br>• Dynamically load only the tools relevant to the current task. | Too many tools confuses the model and slows responses. |\n","| **Prompt engineering** | 1. **Role** – e.g. “You are a helpful weather assistant.”<br>2. **Instructions** – e.g. “Never guess dates; always call the forecast API.”<br>3. **Clarification** – e.g. “If location is unclear, ask a follow-up question.” | Gives the model a deterministic decision path. |\n","| **Temperature** | • Use `temperature = 0` for deterministic, reproducible calls. | Higher values introduce randomness and break workflows. |\n","| **Validation** | • For high-stakes actions (orders, payments), echo the function call back to the user for confirmation **before** executing. | Prevents costly mistakes. |\n","| **Error handling** | • Catch API failures and bad inputs.<br>• Return structured errors, e.g. `{\"error\": \"City not found\"}`. | Lets the model apologise or re-ask instead of hallucinating. |\n","| **Security** | • Keep credentials in env vars or a secrets manager.<br>• Never log sensitive parameters. | Reduces attack surface and data leaks. |\n","| **Token limits** | • Schemas and descriptions count toward the token budget.<br>• If you’re near the limit, shorten docstrings or split tasks into smaller tool sets. | Prevents truncation and “too many tokens” errors. |\n","\n","> **Rule of thumb:** concise, strongly-typed tool specs + low-temperature prompts = predictable, safe function calling.\n"],"metadata":{"id":"iCFAEK-F-Mx4"}},{"cell_type":"markdown","source":["# ╔═════════ Best-Practice Cheat-Sheet ═══════════╗\n","\n","### Best-Practice Checklist for Gemini Function Calling 🔧\n","\n","| Area | What to Do | Why It Matters |\n","|------|------------|----------------|\n","| **Function & parameter descriptions** | • Write *explicit, unambiguous* docstrings.<br>• Spell out units, ranges, and edge cases. | The model chooses the tool—and its arguments—based on these strings alone. |\n","| **Naming** | • Use descriptive snake-case names: `add_user`, `fetch_fx_rate`.<br>• Avoid spaces, periods, or dashes. | Clear names lower the odds of the wrong tool being picked. |\n","| **Strong typing** | • Annotate every parameter (`int`, `str`, etc.).<br>• For limited choices, declare an **enum**. | Tighter schemas ⇒ fewer invalid calls. |\n","| **Tool selection** | • Keep the *active* tool set ≤ 10-20.<br>• Dynamically load only the tools relevant to the current task. | Too many tools confuses the model and slows responses. |\n","| **Prompt engineering** | 1. **Role** – e.g. “You are a helpful weather assistant.”<br>2. **Instructions** – e.g. “Never guess dates; always call the forecast API.”<br>3. **Clarification** – e.g. “If location is unclear, ask a follow-up question.” | Gives the model a deterministic decision path. |\n","| **Temperature** | • Use `temperature = 0` for deterministic, reproducible calls. | Higher values introduce randomness and break workflows. |\n","| **Validation** | • For high-stakes actions (orders, payments), echo the function call back to the user for confirmation **before** executing. | Prevents costly mistakes. |\n","| **Error handling** | • Catch API failures and bad inputs.<br>• Return structured errors, e.g. `{\"error\": \"City not found\"}`. | Lets the model apologise or re-ask instead of hallucinating. |\n","| **Security** | • Keep credentials in env vars or a secrets manager.<br>• Never log sensitive parameters. | Reduces attack surface and data leaks. |\n","| **Token limits** | • Schemas and descriptions count toward the token budget.<br>• If you’re near the limit, shorten docstrings or split tasks into smaller tool sets. | Prevents truncation and “too many tokens” errors. |\n","\n","> **Rule of thumb:** concise, strongly-typed tool specs + low-temperature prompts = predictable, safe function calling.\n"],"metadata":{"id":"anp0OhBon3dE"}}],"metadata":{"colab":{"collapsed_sections":["hY2NtS3jV56U"],"provenance":[]},"google":{"image_path":"/site-assets/images/share.png","keywords":["examples","googleai","samplecode","python","embed","function"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a9d89ffd59f64fe395154b4beb280cc6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23f17a608c7c4e37aec684e8a85f1ec7","IPY_MODEL_a78c83d884794c49acb441058cc06267"],"layout":"IPY_MODEL_e8ccb34d3740430d9cfd8486651c44db"}},"23f17a608c7c4e37aec684e8a85f1ec7":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_80f78aadcf2e4224bd05445dc815a47e","IPY_MODEL_503d0715882948d4911438578b298ab6"],"layout":"IPY_MODEL_84b380da7ed54e6b9e63a6e4d32d741c"}},"a78c83d884794c49acb441058cc06267":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f88acbd5c144d108fedcdc11c66e9a7","placeholder":"​","style":"IPY_MODEL_39632b5096d347f0833f3ad0c968cd8f","value":"<div style='display:flex;flex-direction:column;align-items:center;justify-content:center;height:100%;'><svg width='120' height='120' viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'>  <path d='M12 2c-4 0-7 3-7 7 0 3 2 4 2 7h10c0-3 2-4 2-7 0-4-3-7-7-7z' fill='#cde5fa' stroke='#333' stroke-width='0.5'/>  <rect x='9' y='18' width='6' height='2' fill='#cde5fa' />  <rect x='10.5' y='21' width='3' height='2' fill='#cde5fa' /></svg><span style='margin-top:8px;font-size:14px;'>Custom colour #cde5fa applied.</span></div>"}},"e8ccb34d3740430d9cfd8486651c44db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80f78aadcf2e4224bd05445dc815a47e":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_97dc12d6a7da453b82251ad5bb3310cd","msg_id":"","outputs":[{"output_type":"stream","name":"stdout","text":["👤 You: turn on the lights\n","\n"]},{"output_type":"stream","name":"stdout","text":["🤖 Gemini: The lights have been turned on.\n","\n"]},{"output_type":"stream","name":"stdout","text":["👤 You: can you make it warm pink the lightbulb color\n","\n"]},{"output_type":"stream","name":"stdout","text":["🤖 Gemini: The lightbulb is now warm pink.\n","\n"]},{"output_type":"stream","name":"stdout","text":["👤 You: make it a very light bl\n","\n"]},{"output_type":"stream","name":"stdout","text":["🤖 Gemini: The lightbulb is now a very light blue.\n","\n"]}]}},"503d0715882948d4911438578b298ab6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a047901fc114592a37da8d5b4ffd0a6","IPY_MODEL_589e9a28925a47628cd636988afe2cbe"],"layout":"IPY_MODEL_b62eff6fec064d55b3951ff1be8b6e42"}},"84b380da7ed54e6b9e63a6e4d32d741c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"1 1 0%","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f88acbd5c144d108fedcdc11c66e9a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":"1px solid #ccc","bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"320px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"220px"}},"39632b5096d347f0833f3ad0c968cd8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a047901fc114592a37da8d5b4ffd0a6":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_eb6f8efeaa784aa6af9637ec412ed6a9","placeholder":"Type your message and press Enter…","style":"IPY_MODEL_2f7956f884564b3e9539fe5ca6d380df","value":""}},"589e9a28925a47628cd636988afe2cbe":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"primary","description":"Send","disabled":false,"icon":"","layout":"IPY_MODEL_bf8911c2d422444e9a3f1acfb6b4e0cb","style":"IPY_MODEL_6e610afa99324dba8ce5d77fa725fba6","tooltip":""}},"b62eff6fec064d55b3951ff1be8b6e42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb6f8efeaa784aa6af9637ec412ed6a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"1 1 auto","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f7956f884564b3e9539fe5ca6d380df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf8911c2d422444e9a3f1acfb6b4e0cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e610afa99324dba8ce5d77fa725fba6":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"97dc12d6a7da453b82251ad5bb3310cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":"1px solid #ccc","bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"320px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":"auto","overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}