{"cells":[{"cell_type":"markdown","source":["# Before Starting\n","\n","Go to https://aistudio.google.com/app/apikey copy generative language client free tier API key\n","\n","After you did that click the key icon at the left sidebar add your API key with GOOGLE_API_KEY as name and your API key as the value and enable notebook access\n"],"metadata":{"id":"9l8Tz46_qgSi"}},{"cell_type":"markdown","metadata":{"id":"yeadDkMiISin"},"source":["# Part 2: Gemini API & Function calling with Python"]},{"cell_type":"markdown","metadata":{"id":"df1767a3d1cc"},"source":[" Function calling lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with. Function calling lets you use functions as tools in generative AI applications, and you can define more than one function within a single request.\n","\n","This notebook provides code examples to help you get started. The documentation's [quickstart](https://ai.google.dev/gemini-api/docs/function-calling#python) is also a good place to start understanding function calling."]},{"cell_type":"markdown","metadata":{"id":"hY2NtS3jV56U"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"d5027929de8f"},"source":["### Install dependencies"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"9OEoeosRTv-5","executionInfo":{"status":"ok","timestamp":1750945659638,"user_tz":-180,"elapsed":6336,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[],"source":["%pip install -qU 'google-genai>=1.0.0'"]},{"cell_type":"markdown","metadata":{"id":"x-hHZfLZ7FfH"},"source":["### Set up your API key\n","\n","To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](../quickstarts/Authentication.ipynb) quickstart for an example."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ab9ASynfcIZn","executionInfo":{"status":"ok","timestamp":1750945660243,"user_tz":-180,"elapsed":602,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[],"source":["from google import genai\n","from google.colab import userdata\n","\n","GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n","client = genai.Client(api_key=GOOGLE_API_KEY)"]},{"cell_type":"markdown","metadata":{"id":"zpaKynP8qLw1"},"source":["### Choose a model\n","\n","Function calling should work with all the [Gemini 2.0](https://ai.google.dev/gemini-api/docs/models/gemini-v2) models with the GenAI SDK. It also works with the 1.5 generation of models."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"sEK4ZDVGqJ5H","executionInfo":{"status":"ok","timestamp":1750945773552,"user_tz":-180,"elapsed":9,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[],"source":["MODEL_ID=\"gemini-2.0-flash-lite\" # @param [\"gemini-2.5-flash-preview-05-20\", \"gemini-2.5-pro-preview-03-25\", \"gemini-2.0-flash\", \"gemini-2.0-flash-lite\"] {\"allow-input\":true, isTemplate: true}"]},{"cell_type":"markdown","metadata":{"id":"3f383614ec30"},"source":["## Setting up Functions as Tools"]},{"cell_type":"markdown","metadata":{"id":"b82c1aecb657"},"source":["To use function calling, pass a list of functions to the `tools` parameter when creating a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel). The model uses the function name, docstring, parameters, and parameter type annotations to decide if it needs the function to best answer a prompt.\n","\n","> Important: The SDK converts function parameter type annotations to a format the API understands (`genai.types.FunctionDeclaration`). The API only supports a limited selection of parameter types, and the Python SDK's automatic conversion only supports a subset of that: `AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict`\n","\n","\n","**Example: Lighting System Functions**\n","\n","Here are 3 functions controlling a hypothetical lighting system. Note the docstrings and type hints."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"C8J_H1hSp4m-","executionInfo":{"status":"ok","timestamp":1750945794620,"user_tz":-180,"elapsed":4,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[],"source":["def enable_lights():\n","    \"\"\"Turn on the lighting system.\"\"\"\n","    print(\"LIGHTBOT: Lights enabled.\")\n","\n","\n","def set_light_color(rgb_hex: str):\n","    \"\"\"Set the light color. Lights must be enabled for this to work.\"\"\"\n","    print(f\"LIGHTBOT: Lights set to {rgb_hex}.\")\n","\n","def stop_lights():\n","    \"\"\"Stop flashing lights.\"\"\"\n","    print(\"LIGHTBOT: Lights turned off.\")\n","\n","light_controls = [enable_lights, set_light_color, stop_lights]\n","instruction = \"\"\"\n","  You are a helpful lighting system bot. You can turn\n","  lights on and off, and you can set the color. Do not perform any\n","  other tasks.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"Ry0JsK405KwS"},"source":["## Basic Function Calling with Chat"]},{"cell_type":"markdown","metadata":{"id":"9l4wdq8b5Nuy"},"source":["Function calls naturally fit into multi-turn conversations. The Python SDK's `ChatSession (client.chats.create(...))` is ideal for this, as it automatically handles conversation history.\n","\n","Furthermore, `ChatSession` simplifies function calling execution via its `automatic_function_calling` feature (enabled by default), which will be explored more later. For now, let's see a basic interaction where the model decides to call a function."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"-yuQ2gCY5ujD","outputId":"0204be9b-58c3-4871-867a-1aa4bdf08dab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750945811070,"user_tz":-180,"elapsed":1182,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LIGHTBOT: Lights enabled.\n","Alright, I've turned on the lights.\n","\n"]}],"source":["chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": light_controls,\n","        \"system_instruction\": instruction,\n","    }\n",")\n","\n","response = chat.send_message(\"It's awful dark in here...\")\n","\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"q1UsMG3FqYrC"},"source":["## Examining Function Calls and Execution History\n","\n","To understand what happened in the background, you can examine the chat history.\n","\n","The `Chat.history` property stores a chronological record of the conversation between the user and the Gemini model. You can get the history using `Chat.get_history()`. Each turn in the conversation is represented by a `genai.types.Content` object, which contains the following information:\n","\n","**Role**: Identifies whether the content originated from the \"user\" or the \"model\".\n","\n","**Parts**: A list of genai.types.Part objects that represent individual components of the message. With a text-only model, these parts can be:\n","\n","* **Text**: Plain text messages.\n","* **Function Call (genai.types.FunctionCall)**: A request from the model to execute a specific function with provided arguments.\n","* **Function Response (genai.types.FunctionResponse)**: The result returned by the user after executing the requested function.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBNAqSexqZlZ","outputId":"a88ea142-a2ca-4b40-b40b-27c43c28f2ca","colab":{"base_uri":"https://localhost:8080/","height":335},"executionInfo":{"status":"ok","timestamp":1750543337757,"user_tz":-180,"elapsed":20,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"It's awful dark in here..."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function call: { id=None args={} name='enable_lights' }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###user:"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Function response: { will_continue=None scheduling=None id=None name='enable_lights' response={'result': None} }\n","--------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"###model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Ok, I turned the lights on.\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n"]}],"source":["from IPython.display import Markdown, display\n","\n","def print_history(chat):\n","  for content in chat.get_history():\n","      display(Markdown(\"###\" + content.role + \":\"))\n","      for part in content.parts:\n","          if part.text:\n","              display(Markdown(part.text))\n","          if part.function_call:\n","              print(\"Function call: {\", part.function_call, \"}\")\n","          if part.function_response:\n","              print(\"Function response: {\", part.function_response, \"}\")\n","      print(\"-\" * 80)\n","\n","print_history(chat)"]},{"cell_type":"markdown","metadata":{"id":"CS84-2yG7A--"},"source":["This history shows the flow:\n","\n","1. **User**: Sends the message.\n","\n","2. **Model**: Responds not with text, but with a `FunctionCall` requesting `enable_lights`.\n","\n","3. **User (SDK)**: The `ChatSession` automatically executes `enable_lights()` because `automatic_function_calling` is enabled. It sends the result back as a `FunctionResponse`.\n","\n","4. **Model**: Uses the function's result (\"Lights enabled.\") to formulate the final text response."]},{"cell_type":"markdown","metadata":{"id":"CsCZArT47p5T"},"source":["## Automatic Function Execution (Python SDK Feature)\n","\n","As demonstrated above, the `ChatSession` in the Python SDK has a powerful feature called Automatic Function Execution. When enabled (which it is by default), if the model responds with a FunctionCall, the SDK will:\n","\n","1. Find the corresponding Python function in the provided `tools`.\n","\n","2. Execute the function with the arguments provided by the model.\n","\n","3. Send the function's return value back to the model in a `FunctionResponse`.\n","\n","4. Return only the model's final response (usually text) to your code.\n","\n","This significantly simplifies the workflow for common use cases.\n","\n","**Example: Math Operations**"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"r1FnK3EB8jgQ","outputId":"374994fd-426a-4eda-f3df-180e243336a7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750945977272,"user_tz":-180,"elapsed":2785,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Let $C$ be the number of cats, which is 5567.\n","Let $M$ be the number of mittens each cat owns, which is 443434.\n","We want to find the total number of mittens. To do this, we multiply the number of cats by the number of mittens each cat owns.\n","Total number of mittens = $C \\times M$\n","Total number of mittens = $5567 \\times 443434$\n","To calculate this, we can perform the multiplication:\n","$5567 \\times 443434 = 2468923858$\n","So, the total number of mittens is 2,468,923,858.\n","\n","We can calculate the product as follows:\n","\\begin{array}{@{}c@{\\,}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c}\n","& & & & 4 & 4 & 3 & 4 & 3 & 4 \\\\\n","& & & \\times & & 5 & 5 & 6 & 7 \\\\\n","\\hline\n","& & & 3 & 1 & 0 & 4 & 0 & 3 & 8 \\\\\n","& & 2 & 6 & 6 & 0 & 6 & 0 & 4 & 0 \\\\\n","& 2 & 2 & 1 & 7 & 1 & 7 & 0 & 0 & \\\\\n","2 & 2 & 1 & 7 & 1 & 7 & 0 & 0 & 0 & 0 \\\\\n","\\hline\n","2 & 4 & 6 & 8 & 9 & 2 & 3 & 8 & 5 & 8 \\\\\n","\\end{array}\n","The total number of mittens is 2,468,923,858.\n","\n","Final Answer: The final answer is $\\boxed{2468923858}$\n"]}],"source":["from google.genai import types # Ensure types is imported\n","\n","def add(a: float, b: float):\n","    \"\"\"returns a + b.\"\"\"\n","    return a + b\n","\n","def subtract(a: float, b: float):\n","    \"\"\"returns a - b.\"\"\"\n","    return a - b\n","\n","def multiply(a: float, b: float):\n","    \"\"\"returns a * b.\"\"\"\n","    return a * b\n","\n","def divide(a: float, b: float):\n","    \"\"\"returns a / b.\"\"\"\n","    if b == 0:\n","        return \"Cannot divide by zero.\"\n","    return a / b\n","\n","operation_tools = [add, subtract, multiply, divide]\n","\n","chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"automatic_function_calling\": {\"disable\": False} # Enabled by default\n","    }\n",")\n","\n","response = chat.send_message(\n","    \"I have 5567 cats, each owns 443434 mittens, how many mittens is that in total?\"\n",")\n","\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"a8A6qJ668ywT"},"source":["Automatic execution handled the `multiply` call seamlessly."]},{"cell_type":"markdown","metadata":{"id":"1BzsV6MxLnZD"},"source":["## Automatic Function Schema Declaration\n","\n","A key convenience of the Python SDK is its ability to automatically generate the required `FunctionDeclaration` schema from your Python functions. It inspects:\n","\n","- **Function Name**: (`func.__name__`)\n","\n","- **Docstring**: Used for the function's description.\n","\n","- **Parameters**: Names and type annotations (`int`, `str`, `float`, `bool`, `list`, `dict`). Docstrings for parameters (if using specific formats like Google style) can also enhance the description.\n","\n","- **Return Type Annotation**: Although not strictly used by the model for deciding which function to call, it's good practice.\n","\n","You generally don't need to create `FunctionDeclaration` objects manually when using Python functions directly as tools.\n","\n","However, you can generate the schema explicitly using `genai.types.FunctionDeclaration.from_callable` if you need to inspect it, modify it, or use it in scenarios where you don't have the Python function object readily available."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"qrYRieAuL2hs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750946002442,"user_tz":-180,"elapsed":7,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"44c74f3e-d30a-4ac1-9682-31220c0779d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","    \"description\": \"Set the light color. Lights must be enabled for this to work.\",\n","    \"name\": \"set_light_color\",\n","    \"parameters\": {\n","        \"properties\": {\n","            \"rgb_hex\": {\n","                \"type\": \"STRING\"\n","            }\n","        },\n","        \"required\": [\n","            \"rgb_hex\"\n","        ],\n","        \"type\": \"OBJECT\"\n","    }\n","}\n"]}],"source":["import json\n","\n","set_color_declaration = types.FunctionDeclaration.from_callable(\n","    callable = set_light_color,\n","    client = client\n",")\n","\n","print(json.dumps(set_color_declaration.to_json_dict(), indent=4))"]},{"cell_type":"markdown","metadata":{"id":"EuwKoNIhGBJN"},"source":["## Parallel function calls\n","\n","The Gemini API can call multiple functions in a single turn. This caters for scenarios where there are multiple function calls that can take place independently to complete a task.\n","\n","First set the tools up. Unlike the movie example above, these functions do not require input from each other to be called so they should be good candidates for parallel calling."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"cJ-mSixWGqLv","executionInfo":{"status":"ok","timestamp":1750946083150,"user_tz":-180,"elapsed":12,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[],"source":["def power_disco_ball(power: bool) -> bool:\n","    \"\"\"Powers the spinning disco ball.\"\"\"\n","    print(f\"Disco ball is {'spinning!' if power else 'stopped.'}\")\n","    return True\n","\n","def start_music(energetic: bool, loud: bool, bpm: int) -> str:\n","    \"\"\"Play some music matching the specified parameters.\n","\n","    Args:\n","      energetic: Whether the music is energetic or not.\n","      loud: Whether the music is loud or not.\n","      bpm: The beats per minute of the music.\n","\n","    Returns: The name of the song being played.\n","    \"\"\"\n","    print(f\"Starting music! {energetic=} {loud=}, {bpm=}\")\n","    return \"Never gonna give you up.\"\n","\n","\n","def dim_lights(brightness: float) -> bool:\n","    \"\"\"Dim the lights.\n","\n","    Args:\n","      brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n","    \"\"\"\n","    print(f\"Lights are now set to {brightness:.0%}\")\n","    return True\n","\n","house_fns = [power_disco_ball, start_music, dim_lights]"]},{"cell_type":"markdown","metadata":{"id":"zlrmXN7fxQi0"},"source":["Now call the model with an instruction that could use all of the specified tools."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"21ecYHLgIsCl","colab":{"base_uri":"https://localhost:8080/","height":731},"executionInfo":{"status":"error","timestamp":1750946108071,"user_tz":-180,"elapsed":9193,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"a0d13ca5-ed38-4ff1-a060-6eb9287681d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Disco ball is spinning!\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n","Disco ball is spinning!\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n","Disco ball is spinning!\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n","Disco ball is spinning!\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n","Disco ball is spinning!\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n","Disco ball is spinning!\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n","Disco ball is spinning!\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n","Disco ball is spinning!\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n","Disco ball is spinning!\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n","Disco ball is spinning!\n","Lights are now set to 20%\n","Starting music! energetic=True loud=True, bpm=120\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'print_history' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-18-1786549514.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty_chat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'print_history' is not defined"]}],"source":["# You generally set \"mode\": \"any\" to make sure Gemini actually *uses* the given tools.\n","party_chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": house_fns,\n","        \"tool_config\" : {\n","            \"function_calling_config\": {\n","                \"mode\": \"any\"\n","            }\n","        }\n","    }\n",")\n","\n","# Call the API\n","response = party_chat.send_message(\n","    \"Turn this place into a party!\"\n",")\n","\n","\n","print_history(party_chat)"]},{"cell_type":"markdown","metadata":{"id":"t6iYpty7yZct"},"source":["Notice the single model turn contains three FunctionCall parts, which the SDK then executed before getting the final text response."]},{"cell_type":"markdown","metadata":{"id":"TxXGT3n4AQhk"},"source":["## Compositional Function Calling\n","The model can chain function calls across multiple turns, using the result from one call to inform the next. This allows for complex, multi-step reasoning and task completion.\n","\n","**Example: Finding Specific Movie Showtimes**\n","\n","Let's reuse the theater_functions and ask a more complex query that requires finding movies first, then potentially theaters, then showtimes."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"1jGiexKsAolU","outputId":"f36bcd01-8491-4444-8b4d-26929ad6fa55","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750946189218,"user_tz":-180,"elapsed":2897,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tool Call: get_weather_forecast(location=London)\n","Tool Response: {'temperature': 25, 'unit': 'celsius'}\n","Tool Call: set_thermostat_temperature(temperature=18)\n","Tool Response: {'status': 'success'}\n","OK, I've set the thermostat to 18°C.\n","\n"]}],"source":["import os\n","\n","from google.colab import userdata\n","GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY2')\n","\n","# Example Functions\n","def get_weather_forecast(location: str) -> dict:\n","    \"\"\"Gets the current weather temperature for a given location.\"\"\"\n","    print(f\"Tool Call: get_weather_forecast(location={location})\")\n","    # TODO: Make API call\n","    print(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\")\n","    return {\"temperature\": 16, \"unit\": \"celsius\"}  # Dummy response\n","\n","def set_thermostat_temperature(temperature: int) -> dict:\n","    \"\"\"Sets the thermostat to a desired temperature.\"\"\"\n","    print(f\"Tool Call: set_thermostat_temperature(temperature={temperature})\")\n","\n","    print(\"Tool Response: {'status': 'success'}\")\n","    return {\"status\": \"success\"}\n","\n","client = genai.Client(api_key = GOOGLE_API_KEY)\n","config = types.GenerateContentConfig(\n","    tools=[get_weather_forecast, set_thermostat_temperature]\n",")\n","\n","\n","response = client.models.generate_content(\n","    model=\"gemini-2.0-flash\",\n","    contents=\"If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.\",\n","    config=config,\n",")\n","\n","\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"s7J-nyTN8hL-"},"source":["Here you can see that the model made conditional calls.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"mggqLU55CZlj","colab":{"base_uri":"https://localhost:8080/","height":371,"referenced_widgets":["7756e5ba935b42d694284fff064eca4b","2024ed470d6748308778b7235c770894","4a2025c73a824f238e632fe11f4a69fb","f17a6254de014948a0181ed3cc77ed49","ffa2ee547f2c4f19a64ec802ecd4a8ea","36441e664d1c46ada813c34788369bd6","7aa86987d9c445b883d306f951b15895","a55639f06e5d4a5ea277be39793eb6e6","391bc1043bba400988e70540b95cb138","52485109492c490b9ce3f106892b1f96","0f672ee0c0ea417c8d702514611ef693","e6332255a05b4134a743538eefe5044c","ec880f81703647fb982300abdb2cdc9e","95c6c1bb27f346e1af5e464d5f9e5b2d","d4f8c02fba1a4c938e4fff1aba89ea84","06283d4b6db44a339f3f931409f1d94f","483861ad25d44d619042771131234ab5"]},"executionInfo":{"status":"ok","timestamp":1750946300418,"user_tz":-180,"elapsed":1123,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"48c23ddb-774a-44c4-847c-06c85acd99ef"},"outputs":[{"output_type":"display_data","data":{"text/plain":["HBox(children=(VBox(children=(Output(layout=Layout(border='1px solid #ccc', height='320px', overflow='auto')),…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7756e5ba935b42d694284fff064eca4b"}},"metadata":{}}],"source":["\n","import re\n","from IPython.display import display\n","import ipywidgets as widgets\n","import google.generativeai as genai\n","from google.colab import userdata\n","\n","GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY2')\n","\n","chat_output = widgets.Output(\n","    layout=widgets.Layout(border=\"1px solid #ccc\", height=\"320px\", overflow=\"auto\")\n",")\n","user_input = widgets.Text(\n","    placeholder=\"Type your message and press Enter…\",\n","    layout=widgets.Layout(flex=\"1 1 auto\")\n",")\n","send_button = widgets.Button(description=\"Send\", button_style=\"primary\")\n","\n","SVG_TEMPLATE = (\n","    \"<div style='display:flex;flex-direction:column;align-items:center;justify-content:center;height:100%;'>\"\n","    \"<svg width='120' height='120' viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'>\"\n","    \"  <path d='M12 2c-4 0-7 3-7 7 0 3 2 4 2 7h10c0-3 2-4 2-7 0-4-3-7-7-7z' fill='{color}' stroke='#333' stroke-width='0.5'/>\"\n","    \"  <rect x='9' y='18' width='6' height='2' fill='{color}' />\"\n","    \"  <rect x='10.5' y='21' width='3' height='2' fill='{color}' />\"\n","    \"</svg>\"\n","    \"<span style='margin-top:8px;font-size:14px;'>{msg}</span>\"\n","    \"</div>\"\n",")\n","\n","bulb_output = widgets.HTML(\n","    value=SVG_TEMPLATE.format(color=\"#666\", msg=\"The light is OFF.\"),\n","    layout=widgets.Layout(border=\"1px solid #ccc\", width=\"220px\", height=\"320px\")\n",")\n","\n","# ---------------- State ----------------\n","light_is_on = False\n","\n","# Default ON colour (Gold) and palette\n","COLOR_MAP = {\n","    \"gold\": \"#ffd700\",\n","    \"warm\": \"#ffcc66\",\n","    \"white\": \"#ffffe0\",\n","    \"blue\": \"#40a0ff\",\n","    \"green\": \"#4caf50\",\n","    \"red\": \"#ff5252\",\n","    \"purple\": \"#ba68c8\",\n","}\n","current_on_color = COLOR_MAP[\"gold\"]\n","\n","# ---------------- Helper ----------------\n","\n","def _update_bulb(is_on: bool, msg: str):\n","    \"\"\"Re‑render bulb in its current colour plus caption.\"\"\"\n","    global light_is_on\n","    light_is_on = is_on\n","    color = current_on_color if is_on else \"#666\"  # dark grey when off\n","    bulb_output.value = SVG_TEMPLATE.format(color=color, msg=msg)\n","\n","# ---------------- Tool Functions ----------------\n","\n","def turn_on_lights():\n","    if light_is_on:\n","        _update_bulb(True, \"The lights were already ON.\")\n","        return {\"status\": \"The lights were already ON.\"}\n","    _update_bulb(True, \"The lights have been turned ON.\")\n","    return {\"status\": \"The lights have been turned ON.\"}\n","\n","\n","def turn_off_lights():\n","    if not light_is_on:\n","        _update_bulb(False, \"The lights were already OFF.\")\n","        return {\"status\": \"The lights were already OFF.\"}\n","    _update_bulb(False, \"The lights have been turned OFF.\")\n","    return {\"status\": \"The lights have been turned OFF.\"}\n","\n","\n","def set_light_color(color: str):\n","    \"\"\"Change the ON colour. Accepts common names (gold, blue, …) or hex (#RRGGBB).\"\"\"\n","    global current_on_color\n","\n","    color_str = color.strip().lower()\n","    if color_str in COLOR_MAP:\n","        current_on_color = COLOR_MAP[color_str]\n","        status = f\"Colour set to {color_str}.\"\n","    elif re.fullmatch(r\"#?[0-9a-f]{6}\", color_str):\n","        current_on_color = \"#\" + color_str.lstrip(\"#\")\n","        status = f\"Custom colour {current_on_color} applied.\"\n","    else:\n","        status = \"Invalid colour. Available names: \" + \", \".join(COLOR_MAP.keys())\n","        return {\"status\": status}\n","\n","    # If light is ON, re‑render with new colour immediately\n","    _update_bulb(light_is_on, status)\n","    return {\"status\": status}\n","\n","# ---------------- Gemini 2.0 Flash Setup ----------------\n","light_controls = [turn_on_lights, turn_off_lights, set_light_color]\n","model = genai.GenerativeModel(\n","    model_name=\"gemini-2.0-flash\",  # ← upgraded per request\n","    tools=light_controls,\n",")\n","chat = model.start_chat(enable_automatic_function_calling=True)\n","\n","# ---------------- Chat Handling ----------------\n","\n","def _append_chat(role: str, text: str):\n","    with chat_output:\n","        print(f\"{role}: {text}\\n\")\n","\n","@chat_output.capture()\n","def _send(_=None):\n","    prompt = user_input.value.strip()\n","    if not prompt:\n","        return\n","    _append_chat(\"👤 You\", prompt)\n","    user_input.value = \"\"\n","    try:\n","        response = chat.send_message(prompt)\n","        _append_chat(\"🤖 Gemini\", response.text)\n","    except Exception as e:\n","        _append_chat(\"⚠️ Error\", str(e))\n","\n","# ---------------- Bind UI ----------------\n","send_button.on_click(_send)\n","user_input.on_submit(_send)\n","\n","ui = widgets.HBox([\n","    widgets.VBox([chat_output, widgets.HBox([user_input, send_button])], layout=widgets.Layout(flex=\"1 1 0%\")),\n","    bulb_output\n","])\n","\n","display(ui)\n","\n"]},{"cell_type":"markdown","source":["# Exercises"],"metadata":{"id":"g2rNsyqzi_0S"}},{"cell_type":"code","source":["# ——— solution functions ——— define your functions here\n","\n","\n","calc_tools = [add, multiply]\n","chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": calc_tools,\n","        \"system_instruction\": instruction,\n","    }\n",")\n","\n","response = chat.send_message(\"What’s 7 × 8?  Also, what’s 5 + 4?\")\n","\n","print(response.text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l4FHY8wskcUt","executionInfo":{"status":"ok","timestamp":1750544705302,"user_tz":-180,"elapsed":966,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"6311ac73-99d1-499e-ecae-3b46baf90e93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7 times 8 is 56, and 5 plus 4 is 9.\n","\n"]}]},{"cell_type":"code","source":["# ╔═══════════════ Exercise 2 — Temperature Converter (SOLVED) ═══════════════╗\n","import google.generativeai as genai\n","from google.generativeai import types\n","\n","# ——— solution functions ——— define your functions here\n","\n","\n","temp_tools = [f_to_c, c_to_f]\n","chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": temp_tools,\n","        \"system_instruction\": instruction,\n","    }\n",")\n","\n","response = chat.send_message(\"Convert 72 °F to Celsius.\")\n","\n","print(response.text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swARCEYslNbk","executionInfo":{"status":"ok","timestamp":1750544777337,"user_tz":-180,"elapsed":824,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"11bf03af-bb12-4086-b1aa-f623025845a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["72 degrees Fahrenheit is 22.2 degrees Celsius.\n","\n"]}]},{"cell_type":"code","source":["# ╔═══════════════ Exercise 3 — String Helper (SOLVED) ═══════════════╗\n","import google.generativeai as genai\n","from google.generativeai import types\n","\n","# ——— solution functions ——— define your functions here\n","\n","# ——— Gemini wiring & quick demo ———\n","string_tools = [reverse_str, count_vowels]\n","chat = client.chats.create(\n","    model=MODEL_ID,\n","    config={\n","        \"tools\": string_tools,\n","        \"system_instruction\": instruction,\n","    }\n",")\n","\n","\n","response = chat.send_message(\"How many vowels are in 'function calling'? Also give me the text reversed.\")\n","\n","print(response.text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7QUCDHkHlOfk","executionInfo":{"status":"ok","timestamp":1750544842916,"user_tz":-180,"elapsed":1215,"user":{"displayName":"Umur Yıldız","userId":"08906586032293553750"}},"outputId":"3ea7d09f-5561-46aa-bb6e-b1c9bafe01fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 5 vowels in 'function calling'. The reversed text is 'gnillac noitcnuf'.\n","\n"]}]},{"cell_type":"markdown","source":["# ╔═════════ Best-Practice Cheat-Sheet ═══════════╗\n","\n","### Best-Practice Checklist for Gemini Function Calling 🔧\n","\n","| Area | What to Do | Why It Matters |\n","|------|------------|----------------|\n","| **Function & parameter descriptions** | • Write *explicit, unambiguous* docstrings.<br>• Spell out units, ranges, and edge cases. | The model chooses the tool—and its arguments—based on these strings alone. |\n","| **Naming** | • Use descriptive snake-case names: `add_user`, `fetch_fx_rate`.<br>• Avoid spaces, periods, or dashes. | Clear names lower the odds of the wrong tool being picked. |\n","| **Strong typing** | • Annotate every parameter (`int`, `str`, etc.).<br>• For limited choices, declare an **enum**. | Tighter schemas ⇒ fewer invalid calls. |\n","| **Tool selection** | • Keep the *active* tool set ≤ 10-20.<br>• Dynamically load only the tools relevant to the current task. | Too many tools confuses the model and slows responses. |\n","| **Prompt engineering** | 1. **Role** – e.g. “You are a helpful weather assistant.”<br>2. **Instructions** – e.g. “Never guess dates; always call the forecast API.”<br>3. **Clarification** – e.g. “If location is unclear, ask a follow-up question.” | Gives the model a deterministic decision path. |\n","| **Temperature** | • Use `temperature = 0` for deterministic, reproducible calls. | Higher values introduce randomness and break workflows. |\n","| **Validation** | • For high-stakes actions (orders, payments), echo the function call back to the user for confirmation **before** executing. | Prevents costly mistakes. |\n","| **Error handling** | • Catch API failures and bad inputs.<br>• Return structured errors, e.g. `{\"error\": \"City not found\"}`. | Lets the model apologise or re-ask instead of hallucinating. |\n","| **Security** | • Keep credentials in env vars or a secrets manager.<br>• Never log sensitive parameters. | Reduces attack surface and data leaks. |\n","| **Token limits** | • Schemas and descriptions count toward the token budget.<br>• If you’re near the limit, shorten docstrings or split tasks into smaller tool sets. | Prevents truncation and “too many tokens” errors. |\n","\n","> **Rule of thumb:** concise, strongly-typed tool specs + low-temperature prompts = predictable, safe function calling.\n"],"metadata":{"id":"anp0OhBon3dE"}}],"metadata":{"colab":{"collapsed_sections":["hY2NtS3jV56U"],"provenance":[{"file_id":"https://github.com/google-gemini/cookbook/blob/main/quickstarts/Function_calling.ipynb","timestamp":1750102905409}]},"google":{"image_path":"/site-assets/images/share.png","keywords":["examples","googleai","samplecode","python","embed","function"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7756e5ba935b42d694284fff064eca4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2024ed470d6748308778b7235c770894","IPY_MODEL_4a2025c73a824f238e632fe11f4a69fb"],"layout":"IPY_MODEL_f17a6254de014948a0181ed3cc77ed49"}},"2024ed470d6748308778b7235c770894":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ffa2ee547f2c4f19a64ec802ecd4a8ea","IPY_MODEL_36441e664d1c46ada813c34788369bd6"],"layout":"IPY_MODEL_7aa86987d9c445b883d306f951b15895"}},"4a2025c73a824f238e632fe11f4a69fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a55639f06e5d4a5ea277be39793eb6e6","placeholder":"​","style":"IPY_MODEL_391bc1043bba400988e70540b95cb138","value":"<div style='display:flex;flex-direction:column;align-items:center;justify-content:center;height:100%;'><svg width='120' height='120' viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'>  <path d='M12 2c-4 0-7 3-7 7 0 3 2 4 2 7h10c0-3 2-4 2-7 0-4-3-7-7-7z' fill='#666' stroke='#333' stroke-width='0.5'/>  <rect x='9' y='18' width='6' height='2' fill='#666' />  <rect x='10.5' y='21' width='3' height='2' fill='#666' /></svg><span style='margin-top:8px;font-size:14px;'>The light is OFF.</span></div>"}},"f17a6254de014948a0181ed3cc77ed49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffa2ee547f2c4f19a64ec802ecd4a8ea":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_483861ad25d44d619042771131234ab5","msg_id":"","outputs":[]}},"36441e664d1c46ada813c34788369bd6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52485109492c490b9ce3f106892b1f96","IPY_MODEL_0f672ee0c0ea417c8d702514611ef693"],"layout":"IPY_MODEL_e6332255a05b4134a743538eefe5044c"}},"7aa86987d9c445b883d306f951b15895":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"1 1 0%","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a55639f06e5d4a5ea277be39793eb6e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":"1px solid #ccc","bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"320px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"220px"}},"391bc1043bba400988e70540b95cb138":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52485109492c490b9ce3f106892b1f96":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_ec880f81703647fb982300abdb2cdc9e","placeholder":"Type your message and press Enter…","style":"IPY_MODEL_95c6c1bb27f346e1af5e464d5f9e5b2d","value":""}},"0f672ee0c0ea417c8d702514611ef693":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"primary","description":"Send","disabled":false,"icon":"","layout":"IPY_MODEL_d4f8c02fba1a4c938e4fff1aba89ea84","style":"IPY_MODEL_06283d4b6db44a339f3f931409f1d94f","tooltip":""}},"e6332255a05b4134a743538eefe5044c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec880f81703647fb982300abdb2cdc9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"1 1 auto","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95c6c1bb27f346e1af5e464d5f9e5b2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4f8c02fba1a4c938e4fff1aba89ea84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06283d4b6db44a339f3f931409f1d94f":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"483861ad25d44d619042771131234ab5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":"1px solid #ccc","bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"320px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":"auto","overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}